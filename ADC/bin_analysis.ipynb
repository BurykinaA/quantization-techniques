{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ADC.ste import ste_round, ste_floor # Import from your ste.py\n",
    "from ADC.quantizers import ADCQuantizer, ADCQuantizerAshift\n",
    "from scipy.stats import laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_uniform_core(x, n_bits, round_fn, data_min_val, data_max_val):\n",
    "    \"\"\"\n",
    "    Core uniform quantization logic. Returns dequantized values and integer levels.\n",
    "    \"\"\"\n",
    "    data_min = torch.tensor(data_min_val, device=x.device, dtype=x.dtype)\n",
    "    data_max = torch.tensor(data_max_val, device=x.device, dtype=x.dtype)\n",
    "    num_levels = 2**n_bits\n",
    "\n",
    "    if num_levels <= 1 or (data_max - data_min).abs() < 1e-9:\n",
    "        levels_int = torch.zeros_like(x, dtype=torch.float32) # float for consistency\n",
    "        dequant_val = torch.full_like(x, data_min)\n",
    "        return dequant_val, levels_int\n",
    "\n",
    "    scale = (data_max - data_min) / (num_levels - 1)\n",
    "    \n",
    "    x_clamped_input = torch.clamp(x, data_min, data_max)\n",
    "    x_transformed = (x_clamped_input - data_min) / scale\n",
    "    \n",
    "    quantized_levels_float = round_fn(x_transformed)\n",
    "    quantized_levels_int = torch.clamp(quantized_levels_float, 0, num_levels - 1)\n",
    "    \n",
    "    x_dequant = quantized_levels_int * scale + data_min\n",
    "    # Shift levels for symmetric to be centered around 0 if desired for interpretation\n",
    "    # For this analysis, levels 0 to N-1 are fine.\n",
    "    return x_dequant, quantized_levels_int.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_elementwise(data, n_bits, quant_type, round_fn):\n",
    "    if quant_type == 'affine':\n",
    "        min_val, max_val = data.min().item(), data.max().item()\n",
    "        if max_val <= min_val + 1e-9: max_val = min_val + 1.0\n",
    "    elif quant_type == 'symmetric':\n",
    "        abs_max = data.abs().max().item()\n",
    "        min_val, max_val = -abs_max, abs_max\n",
    "        if max_val <= min_val + 1e-9 : \n",
    "            min_val = -1.0 # Default small range if data is all zero\n",
    "            max_val = 1.0\n",
    "    else:\n",
    "        raise ValueError(\"quant_type must be 'affine' or 'symmetric'\")\n",
    "    \n",
    "    return quantize_uniform_core(data, n_bits, round_fn, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_data(num_samples, mean=0.0, std=0.1):\n",
    "    return torch.randn(num_samples) * std + mean, f\"Normal (μ={mean:.2f}, σ={std:.2f})\"\n",
    "\n",
    "def generate_exponential_data(num_samples, rate=1.0):\n",
    "    return torch.distributions.Exponential(rate).sample((num_samples,)), f\"Exponential (rate={rate:.1f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_product_quantization_comparison(bw=4, bx=4, ba=8, k_adc=4, num_samples=10000):\n",
    "    print(f\"\\n--- Product Quantization Comparison ---\")\n",
    "    print(f\"Params: w_bits={bw}, x_bits={bx}, final_product_bits={ba}, adc_k={k_adc}, Samples={num_samples}\")\n",
    "\n",
    "    w_orig, w_dist_name = generate_normal_data(num_samples, std=0.05)\n",
    "    x_orig, x_dist_name = generate_exponential_data(num_samples, rate=0.5) # Mean = 2\n",
    "\n",
    "    # --- Method 1: Standard Quantization of Product ---\n",
    "    w_std_dequant_for_prod, _ = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=torch.round)\n",
    "    x_std_dequant_for_prod, _ = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=torch.round)\n",
    "    product_for_std_quant = w_std_dequant_for_prod * x_std_dequant_for_prod\n",
    "    # Quantize the product itself to 'ba' bits using affine quantization and standard round\n",
    "    _, product_std_quant_levels = quantize_elementwise(product_for_std_quant, n_bits=ba, quant_type='affine', round_fn=torch.round)\n",
    "\n",
    "    # --- Method 2: ADC-Style (STE-Floor for w,x pre-quant) ---\n",
    "    w_floor_inter_dequant, w_floor_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_floor)\n",
    "    x_floor_inter_dequant, x_floor_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_floor)\n",
    "    product_input_to_adc_floor = w_floor_inter_dequant * x_floor_inter_dequant\n",
    "    adc_module_floor = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) # M=1 for element-wise product\n",
    "    adc_floor_output_levels = adc_module_floor(product_input_to_adc_floor)\n",
    "\n",
    "    # --- Method 3: ADC-Style (STE-Round for w,x pre-quant) ---\n",
    "    w_round_inter_dequant, w_round_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_round)\n",
    "    x_round_inter_dequant, x_round_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_round)\n",
    "    product_input_to_adc_round = w_round_inter_dequant * x_round_inter_dequant\n",
    "    adc_module_round = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_round_output_levels = adc_module_round(product_input_to_adc_round)\n",
    "    \n",
    "    print(\"\\n  --- Product Bin Counts (all target 'ba' bits) ---\")\n",
    "    print(f\"    Standard Product Quant (affine, round) bins: {len(torch.unique(product_std_quant_levels))}\")\n",
    "    print(f\"    ADC Output (pre w,x quant w/ STE-Floor) bins: {len(torch.unique(adc_floor_output_levels))}\")\n",
    "    print(f\"    ADC Output (pre w,x quant w/ STE-Round) bins: {len(torch.unique(adc_round_output_levels))}\")\n",
    "    print(f\"  --- Intermediate Quantized W/X Bin Counts ---\")\n",
    "    print(f\"    Intermediate W (symm, STE-Floor, {bw}-bit) bins: {len(torch.unique(w_floor_inter_levels))}\")\n",
    "    print(f\"    Intermediate X (aff, STE-Floor, {bx}-bit) bins: {len(torch.unique(x_floor_inter_levels))}\")\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 16)) # Now 3 rows\n",
    "    title_str = (f\"Product Quantization Bin Comparison ({ba}-bit output)\\n\"\n",
    "                 f\"Inputs: w ({bw}b-symm), x ({bx}b-aff). ADC M=1, k={k_adc}\")\n",
    "    fig.suptitle(title_str, fontsize=15)\n",
    "\n",
    "    # Row 0: Originals \n",
    "    axes[0, 0].hist(w_orig.cpu().numpy(), bins=100, color='gray', alpha=0.7, density=True)\n",
    "    axes[0, 0].set_title(f\"Original W ({w_dist_name})\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].hist(x_orig.cpu().numpy(), bins=100, color='skyblue', alpha=0.7, density=True)\n",
    "    axes[0, 1].set_title(f\"Original X ({x_dist_name})\")\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 2].axis('off') # Keep one empty for layout or future use\n",
    "\n",
    "    # Helper for plotting levels (bins)\n",
    "    def plot_levels(ax, levels_data, title, color, is_dequant_plot=False):\n",
    "        unique_vals, counts = torch.unique(levels_data, return_counts=True)\n",
    "        bar_width = 0.8 \n",
    "        if len(unique_vals) > 1:\n",
    "            min_diff = (torch.sort(unique_vals).values[1:] - torch.sort(unique_vals).values[:-1]).min().item()\n",
    "            # For dequantized values, which are continuous-like, or many bins, use hist\n",
    "            if is_dequant_plot or len(unique_vals) > 2 * (2**max(bw,bx,ba)): # Heuristic for hist\n",
    "                ax.hist(levels_data.cpu().numpy(), bins=50, color=color, alpha=0.8, density=True)\n",
    "            else: # For few discrete levels, use bar\n",
    "                bar_width = min_diff * 0.8 if min_diff > 1e-6 else 0.8 # Ensure positive width for bars\n",
    "                if bar_width < 1e-5: bar_width = 0.05 * (unique_vals.abs().mean().item() if len(unique_vals)>0 else 1.0)\n",
    "                if bar_width < 1e-5: bar_width = 0.05\n",
    "                ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                       width=bar_width, color=color, alpha=0.85)\n",
    "        elif len(unique_vals) == 1: \n",
    "             bar_width = 0.05 * abs(unique_vals.item()) if abs(unique_vals.item()) > 0 else 0.05\n",
    "             if bar_width == 0: bar_width = 0.05\n",
    "             ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                    width=bar_width, color=color, alpha=0.85)\n",
    "        else: # No data or no unique values\n",
    "            ax.text(0.5, 0.5, \"No data/bins\", ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "        ax.set_title(f\"{title}\\nUnique Bins/Values: {len(unique_vals)}\")\n",
    "        ax.set_ylabel(\"Normalized Freq. / Density\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Row 1: Intermediately Quantized W, X (using STE-Floor for ADC path example) and a Product Input\n",
    "    # Plotting dequantized values here to see their distribution before product\n",
    "    plot_levels(axes[1, 0], w_floor_inter_dequant, f\"Intermed. Quant W ({bw}b, STE-Floor)\\n(Dequantized)\", 'darkblue', is_dequant_plot=True)\n",
    "    plot_levels(axes[1, 1], x_floor_inter_dequant, f\"Intermed. Quant X ({bx}b, STE-Floor)\\n(Dequantized)\", 'darkgreen', is_dequant_plot=True)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "\n",
    "    # Row 2: Final Product Quantized Levels (Bins) from the three methods\n",
    "    plot_levels(axes[2, 0], product_std_quant_levels, f\"Std. Product Quant Levels ({ba}-bit)\\n(Affine, torch.round)\", 'purple')\n",
    "    plot_levels(axes[2, 1], adc_floor_output_levels, f\"ADC Output Levels ({ba}-bit)\\n(Pre w,x STE-Floor)\", 'teal')\n",
    "    plot_levels(axes[2, 2], adc_round_output_levels, f\"ADC Output Levels ({ba}-bit)\\n(Pre w,x STE-Round)\", 'orangered')\n",
    "    \n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlabel(\"Value / Level\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92]) \n",
    "    output_dir = os.path.join(\"ADC\", \"analysis_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_filename = os.path.join(output_dir, f\"product_quant_compare_w{bw}x{bx}_out{ba}_k{k_adc}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"\\nPlot saved to {plot_filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Quantization Comparison ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 144\n",
      "    ADC Output (pre w,x quant w/ STE-Floor) bins: 2\n",
      "    ADC Output (pre w,x quant w/ STE-Round) bins: 2\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 186\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 153\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4.png\n"
     ]
    }
   ],
   "source": [
    "run_product_quantization_comparison(bw=8, bx=8, ba=8, k_adc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_product_quantization_comparison(bw=4, bx=4, ba=8, k_adc=4, num_samples=10000):\n",
    "    print(f\"\\n--- Product Quantization Comparison ---\")\n",
    "    print(f\"Params: w_bits={bw}, x_bits={bx}, final_product_bits={ba}, adc_k={k_adc}, Samples={num_samples}\")\n",
    "\n",
    "    w_orig, w_dist_name = generate_normal_data(num_samples, std=0.05)\n",
    "    x_orig, x_dist_name = generate_exponential_data(num_samples, rate=0.5) # Mean = 2\n",
    "\n",
    "    # --- Method 1: Standard Quantization of Product ---\n",
    "    w_std_dequant_for_prod, _ = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=torch.round)\n",
    "    x_std_dequant_for_prod, _ = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=torch.round)\n",
    "    product_for_std_quant = w_std_dequant_for_prod * x_std_dequant_for_prod\n",
    "    # Quantize the product itself to 'ba' bits using affine quantization and standard round\n",
    "    _, product_std_quant_levels = quantize_elementwise(product_for_std_quant, n_bits=ba, quant_type='affine', round_fn=torch.round)\n",
    "\n",
    "    # --- Method 2: ADC-Style (STE-Floor for w,x pre-quant), Original ADCQuantizer ---\n",
    "    w_floor_inter_dequant, w_floor_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_floor)\n",
    "    x_floor_inter_dequant, x_floor_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_floor)\n",
    "    product_input_to_adc_floor = w_floor_inter_dequant * x_floor_inter_dequant\n",
    "    adc_module_orig_floor = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_orig_floor_output_levels = adc_module_orig_floor(product_input_to_adc_floor)\n",
    "\n",
    "    # --- Method 3: ADC-Style (STE-Round for w,x pre-quant), Original ADCQuantizer ---\n",
    "    w_round_inter_dequant, w_round_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_round)\n",
    "    x_round_inter_dequant, x_round_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_round)\n",
    "    product_input_to_adc_round = w_round_inter_dequant * x_round_inter_dequant\n",
    "    adc_module_orig_round = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_orig_round_output_levels = adc_module_orig_round(product_input_to_adc_round)\n",
    "    \n",
    "    # --- Method 4: ADC-Ashift-Style (STE-Round for w,x pre-quant), ADCQuantizerAshift ---\n",
    "    # Use w_round_inter_dequant and x_round_inter_dequant (calculated for Method 3)\n",
    "    # product_input_to_adc_ashift_round is the same as product_input_to_adc_round\n",
    "    adc_module_ashift_round = ADCQuantizerAshift(M=1, bx=bx, bw=bw, ba=ba, k=k_adc, ashift_enabled=True)\n",
    "    adc_ashift_round_output_levels = adc_module_ashift_round(product_input_to_adc_round) # Use product from STE-Round inputs\n",
    "    \n",
    "    print(\"\\n  --- Product Bin Counts (all target 'ba' bits) ---\")\n",
    "    print(f\"    Standard Product Quant (affine, round) bins: {len(torch.unique(product_std_quant_levels))}\")\n",
    "    print(f\"    Orig. ADC (pre w,x STE-Floor) bins: {len(torch.unique(adc_orig_floor_output_levels))}\")\n",
    "    print(f\"    Orig. ADC (pre w,x STE-Round) bins: {len(torch.unique(adc_orig_round_output_levels))}\")\n",
    "    print(f\"    Ashift ADC (pre w,x STE-Round) bins: {len(torch.unique(adc_ashift_round_output_levels))}\") # Updated name\n",
    "    print(f\"  --- Intermediate Quantized W/X Bin Counts ---\")\n",
    "    print(f\"    Intermediate W (symm, STE-Floor, {bw}-bit) bins: {len(torch.unique(w_floor_inter_levels))}\") # For context for method 2\n",
    "    print(f\"    Intermediate X (aff, STE-Floor, {bx}-bit) bins: {len(torch.unique(x_floor_inter_levels))}\") # For context for method 2\n",
    "    print(f\"    Intermediate W (symm, STE-Round, {bw}-bit) bins: {len(torch.unique(w_round_inter_levels))}\") # For context for methods 3 & 4\n",
    "    print(f\"    Intermediate X (aff, STE-Round, {bx}-bit) bins: {len(torch.unique(x_round_inter_levels))}\") # For context for methods 3 & 4\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(20, 20)) \n",
    "    title_str = (f\"Product Quantization Bin Comparison ({ba}-bit output)\\n\"\n",
    "                 f\"Inputs: w ({bw}b-symm), x ({bx}b-aff). ADC M=1, k={k_adc}\")\n",
    "    fig.suptitle(title_str, fontsize=15)\n",
    "\n",
    "    # Row 0: Originals \n",
    "    axes[0, 0].hist(w_orig.cpu().numpy(), bins=100, color='gray', alpha=0.7, density=True)\n",
    "    axes[0, 0].set_title(f\"Original W ({w_dist_name})\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].hist(x_orig.cpu().numpy(), bins=100, color='skyblue', alpha=0.7, density=True)\n",
    "    axes[0, 1].set_title(f\"Original X ({x_dist_name})\")\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 2].axis('off') \n",
    "\n",
    "    # Helper for plotting levels (bins)\n",
    "    def plot_levels(ax, levels_data, title, color, is_dequant_plot=False):\n",
    "        unique_vals, counts = torch.unique(levels_data, return_counts=True)\n",
    "        bar_width = 0.8 \n",
    "        if len(unique_vals) > 1:\n",
    "            min_diff = (torch.sort(unique_vals).values[1:] - torch.sort(unique_vals).values[:-1]).min().item()\n",
    "            if is_dequant_plot or len(unique_vals) > 2 * (2**max(bw,bx,ba)): \n",
    "                ax.hist(levels_data.cpu().numpy(), bins=50, color=color, alpha=0.8, density=True)\n",
    "            else: \n",
    "                bar_width = min_diff * 0.8 if min_diff > 1e-6 else 0.8 \n",
    "                if bar_width < 1e-5: bar_width = 0.05 * (unique_vals.abs().mean().item() if len(unique_vals)>0 else 1.0)\n",
    "                if bar_width < 1e-5: bar_width = 0.05\n",
    "                ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                       width=bar_width, color=color, alpha=0.85)\n",
    "        elif len(unique_vals) == 1: \n",
    "             bar_width = 0.05 * abs(unique_vals.item()) if abs(unique_vals.item()) > 0 else 0.05\n",
    "             if bar_width == 0: bar_width = 0.05\n",
    "             ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                    width=bar_width, color=color, alpha=0.85)\n",
    "        else: \n",
    "            ax.text(0.5, 0.5, \"No data/bins\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(f\"{title}\\nUnique Bins/Values: {len(unique_vals)}\")\n",
    "        ax.set_ylabel(\"Normalized Freq. / Density\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Row 1: Intermediately Quantized W, X (using STE-Round for ADC Ashift path example) \n",
    "    plot_levels(axes[1, 0], w_round_inter_dequant, f\"Intermed. Quant W ({bw}b, STE-Round)\\n(Dequantized)\", 'darkslateblue', is_dequant_plot=True)\n",
    "    plot_levels(axes[1, 1], x_round_inter_dequant, f\"Intermed. Quant X ({bx}b, STE-Round)\\n(Dequantized)\", 'seagreen', is_dequant_plot=True)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "\n",
    "    # Row 2: Product Quantized Levels (Bins) - Part 1\n",
    "    plot_levels(axes[2, 0], product_std_quant_levels, f\"Std. Product Quant Levels ({ba}-bit)\\n(Affine, torch.round)\", 'purple')\n",
    "    plot_levels(axes[2, 1], adc_orig_floor_output_levels, f\"Orig. ADC Output ({ba}-bit)\\n(Pre w,x STE-Floor)\", 'teal')\n",
    "    axes[2, 2].axis('off') \n",
    "\n",
    "    # Row 3: Product Quantized Levels (Bins) - Part 2\n",
    "    plot_levels(axes[3, 0], adc_orig_round_output_levels, f\"Orig. ADC Output ({ba}-bit)\\n(Pre w,x STE-Round)\", 'orangered')\n",
    "    plot_levels(axes[3, 1], adc_ashift_round_output_levels, f\"Ashift ADC Output ({ba}-bit)\\n(Pre w,x STE-Round)\", 'saddlebrown') # Updated title\n",
    "    axes[3, 2].axis('off')\n",
    "    \n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlabel(\"Value / Level\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92]) \n",
    "    output_dir = os.path.join(\"ADC\", \"analysis_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_filename = os.path.join(output_dir, f\"product_quant_compare_w{bw}x{bx}_out{ba}_k{k_adc}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"\\nPlot saved to {plot_filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Quantization Comparison ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 164\n",
      "    Orig. ADC (pre w,x STE-Floor) bins: 2\n",
      "    Orig. ADC (pre w,x STE-Round) bins: 2\n",
      "    Ashift ADC (pre w,x STE-Round) bins: 2\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 202\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 175\n",
      "    Intermediate W (symm, STE-Round, 8-bit) bins: 199\n",
      "    Intermediate X (aff, STE-Round, 8-bit) bins: 176\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4.png\n"
     ]
    }
   ],
   "source": [
    "run_product_quantization_comparison(bw=8, bx=8, ba=8, k_adc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_laplace_data_for_weights(num_samples, loc=0.0, scale=0.03): # scale is b for Laplace\n",
    "    # Scale chosen to have a somewhat comparable spread to normal std=0.05\n",
    "    # Variance of Laplace is 2*scale^2. Std dev = sqrt(2)*scale.\n",
    "    # For std_dev ~ 0.05, sqrt(2)*scale = 0.05 => scale = 0.05/sqrt(2) ~ 0.035\n",
    "    return torch.tensor(laplace.rvs(loc=loc, scale=scale, size=num_samples), dtype=torch.float32), \\\n",
    "           f\"Laplace Weights (loc={loc:.2f}, scale={scale:.3f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clipped_sum_normals_data_for_weights(num_samples, \n",
    "                                                 mean1=-0.05, std1=0.03, \n",
    "                                                 mean2=0.05, std2=0.03, \n",
    "                                                 clip_std_factor=1.5):\n",
    "    n_half = num_samples // 2\n",
    "    dist1 = torch.randn(n_half) * std1 + mean1\n",
    "    dist2 = torch.randn(num_samples - n_half) * std2 + mean2\n",
    "    \n",
    "    # Clip tails\n",
    "    clip_min1, clip_max1 = mean1 - clip_std_factor * std1, mean1 + clip_std_factor * std1\n",
    "    clip_min2, clip_max2 = mean2 - clip_std_factor * std2, mean2 + clip_std_factor * std2\n",
    "    \n",
    "    dist1_clipped = torch.clamp(dist1, clip_min1, clip_max1)\n",
    "    dist2_clipped = torch.clamp(dist2, clip_min2, clip_max2)\n",
    "    \n",
    "    # For sum, we can take samples from each and sum, or just create a mixed distribution\n",
    "    # The description implies \"sum of 2 normal that after the sum will not have their left and right tails\"\n",
    "    # This interpretation is a bit ambiguous. Let's create a bimodal-like distribution by concatenating\n",
    "    # two normals that are themselves somewhat \"flattened\" by clipping, or more directly,\n",
    "    # a sum that results in a flatter top.\n",
    "    # A simpler approach to get a \"flatter\" distribution is to reduce kurtosis.\n",
    "    # For this example, let's try a sum that might spread out.\n",
    "    # Or, to ensure no extreme tails after sum, we could sum and then clip the sum.\n",
    "    # Let's try a bimodal by concatenation of two slightly offset distributions,\n",
    "    # which might not be exactly \"sum of 2 normals\" but achieves a flatter, wider shape.\n",
    "    # Re-interpreting: \"sum of 2 normal that after the sum will not have their left and right tails\"\n",
    "    # This could mean (N1_clipped + N2_clipped) / 2 or similar.\n",
    "    # For simplicity and to ensure a different shape: let's try a mixture.\n",
    "    \n",
    "    # Alternative interpretation for \"sum of 2 normal that after the sum will not have their left and right tails\"\n",
    "    # Generate N(0, sigma_base), then transform it to be flatter.\n",
    "    # For now, let's make a bimodal by concatenating two slightly offset distributions.\n",
    "    # This aims for a distribution with more mass at the edges of a central region.\n",
    "    offset = 0.06\n",
    "    std_base = 0.02\n",
    "    w1 = torch.randn(n_half) * std_base - offset\n",
    "    w2 = torch.randn(num_samples - n_half) * std_base + offset\n",
    "    weights = torch.cat((w1, w2))\n",
    "    # Further clip the combined distribution to ensure no extreme tails after forming bimodal\n",
    "    final_clip_val = offset + 2.5 * std_base\n",
    "    weights = torch.clamp(weights, -final_clip_val, final_clip_val)\n",
    "\n",
    "    return weights, f\"Bimodal-like Weights (No extreme tails)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_u_shaped_weights(\n",
    "    num_samples: int,\n",
    "    center_gap: float = 5.0,\n",
    "    spread: float = 4.0,\n",
    "    noise_std: float = 0.2,\n",
    "    middle_ratio: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерирует U-образное распределение с двумя \"пиками\" по краям \n",
    "    и достаточно равномерными данными между ними.\n",
    "\n",
    "    Параметры:\n",
    "        num_samples   — общее число точек\n",
    "        center_gap    — полуширина пустой зоны (зазора) вокруг 0\n",
    "        spread        — расширение от края зазора до пиков\n",
    "        noise_std     — стандартное отклонение добавляемого шума\n",
    "        middle_ratio  — доля точек, попадающих в середину (между пиками)\n",
    "\n",
    "    Возвращает:\n",
    "        weights       — Tensor размера (num_samples,) с U-образным распределением\n",
    "        description   — строка с описанием распределения\n",
    "    \"\"\"\n",
    "    # сколько точек в средней (равномерной) зоне\n",
    "    n_middle = int(num_samples * middle_ratio)\n",
    "    # остаток делим пополам для левой и правой части\n",
    "    n_side = num_samples - n_middle\n",
    "    n_left  = n_side // 2\n",
    "    n_right = n_side - n_left\n",
    "\n",
    "    # левая часть: uniform на [-(spread+center_gap), -center_gap]\n",
    "    left = torch.empty(n_left).uniform_(-spread - center_gap, -center_gap)\n",
    "    # средняя часть: uniform на [-center_gap, +center_gap]\n",
    "    middle = torch.empty(n_middle).uniform_(-center_gap, center_gap)\n",
    "    # правая часть: uniform на [center_gap, spread+center_gap]\n",
    "    right = torch.empty(n_right).uniform_(center_gap, spread + center_gap)\n",
    "\n",
    "    # объединяем и добавляем мелкий шум\n",
    "    weights = torch.cat([left, middle, right])\n",
    "    weights += torch.randn_like(weights) * noise_std\n",
    "\n",
    "    description = \"U-shaped Bimodal Distribution with Uniform Middle\"\n",
    "    return weights, description\n",
    "\n",
    "# Пример использования:\n",
    "# weights, desc = generate_u_shaped_weights(10000, middle_ratio=0.4)\n",
    "# plt.hist(weights.numpy(), bins=300)\n",
    "# plt.title(desc)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights, description = generate_u_shaped_weights(10000)\n",
    "plt.hist(weights.numpy(), bins=300)\n",
    "plt.title(description)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exponential_data_for_activations(num_samples, rate=1.0): # Renamed for clarity\n",
    "    return torch.distributions.Exponential(rate).sample((num_samples,)), f\"Exponential Activations (rate={rate:.1f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_data_for_weights(num_samples, mean=0.0, std=0.05): # Renamed for clarity\n",
    "    return torch.randn(num_samples) * std + mean, f\"Normal Weights (μ={mean:.2f}, σ={std:.2f})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_product_quantization_comparison(w_generator_fn, bw=4, bx=4, ba=8, k_adc=4, num_samples=10000, plot_suffix=\"\"):\n",
    "    print(f\"\\n--- Product Quantization Comparison ({plot_suffix.replace('_', ' ')}) ---\")\n",
    "    print(f\"Params: w_bits={bw}, x_bits={bx}, final_product_bits={ba}, adc_k={k_adc}, Samples={num_samples}\")\n",
    "\n",
    "    w_orig, w_dist_name = w_generator_fn(num_samples)\n",
    "    x_orig, x_dist_name = generate_exponential_data_for_activations(num_samples, rate=0.5) # Mean = 2\n",
    "\n",
    "    # --- Method 1: Standard Quantization of Product ---\n",
    "    w_std_dequant_for_prod, _ = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=torch.round)\n",
    "    x_std_dequant_for_prod, _ = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=torch.round)\n",
    "    product_for_std_quant = w_std_dequant_for_prod * x_std_dequant_for_prod\n",
    "    _, product_std_quant_levels = quantize_elementwise(product_for_std_quant, n_bits=ba, quant_type='affine', round_fn=torch.round)\n",
    "\n",
    "    # --- Method 2: ADC-Style (STE-Floor for w,x pre-quant), Original ADCQuantizer ---\n",
    "    w_floor_inter_dequant, w_floor_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_floor)\n",
    "    x_floor_inter_dequant, x_floor_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_floor)\n",
    "    product_input_to_adc_floor = w_floor_inter_dequant * x_floor_inter_dequant\n",
    "    adc_module_orig_floor = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_orig_floor_output_levels = adc_module_orig_floor(product_input_to_adc_floor)\n",
    "\n",
    "    # --- Method 3: ADC-Style (STE-Round for w,x pre-quant), Original ADCQuantizer ---\n",
    "    w_round_inter_dequant, w_round_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_round)\n",
    "    x_round_inter_dequant, x_round_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_round)\n",
    "    product_input_to_adc_round = w_round_inter_dequant * x_round_inter_dequant\n",
    "    adc_module_orig_round = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_orig_round_output_levels = adc_module_orig_round(product_input_to_adc_round)\n",
    "    \n",
    "    # --- Method 4: ADC-Ashift-Style (STE-Round for w,x pre-quant), ADCQuantizerAshift ---\n",
    "    adc_module_ashift_round = ADCQuantizerAshift(M=1, bx=bx, bw=bw, ba=ba, k=k_adc, ashift_enabled=True)\n",
    "    adc_ashift_round_output_levels = adc_module_ashift_round(product_input_to_adc_round) \n",
    "    \n",
    "    print(\"\\n  --- Product Bin Counts (all target 'ba' bits) ---\")\n",
    "    print(f\"    Standard Product Quant (affine, round) bins: {len(torch.unique(product_std_quant_levels))}\")\n",
    "    print(f\"    Orig. ADC (pre w,x STE-Floor) bins: {len(torch.unique(adc_orig_floor_output_levels))}\")\n",
    "    print(f\"    Orig. ADC (pre w,x STE-Round) bins: {len(torch.unique(adc_orig_round_output_levels))}\")\n",
    "    print(f\"    Ashift ADC (pre w,x STE-Round) bins: {len(torch.unique(adc_ashift_round_output_levels))}\")\n",
    "    print(f\"  --- Intermediate Quantized W/X Bin Counts ---\")\n",
    "    print(f\"    Intermediate W (symm, STE-Floor, {bw}-bit) bins: {len(torch.unique(w_floor_inter_levels))}\")\n",
    "    print(f\"    Intermediate X (aff, STE-Floor, {bx}-bit) bins: {len(torch.unique(x_floor_inter_levels))}\")\n",
    "    print(f\"    Intermediate W (symm, STE-Round, {bw}-bit) bins: {len(torch.unique(w_round_inter_levels))}\")\n",
    "    print(f\"    Intermediate X (aff, STE-Round, {bx}-bit) bins: {len(torch.unique(x_round_inter_levels))}\")\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(20, 20)) \n",
    "    title_str = (f\"Product Quantization ({plot_suffix.replace('_', ' ')}) ({ba}-bit output)\\n\"\n",
    "                 f\"Inputs: w ({bw}b-symm), x ({bx}b-aff). ADC M=1, k={k_adc}\")\n",
    "    fig.suptitle(title_str, fontsize=15)\n",
    "\n",
    "    axes[0, 0].hist(w_orig.cpu().numpy(), bins=100, color='gray', alpha=0.7, density=True)\n",
    "    axes[0, 0].set_title(f\"Original W ({w_dist_name})\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].hist(x_orig.cpu().numpy(), bins=100, color='skyblue', alpha=0.7, density=True)\n",
    "    axes[0, 1].set_title(f\"Original X ({x_dist_name})\")\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 2].axis('off') \n",
    "\n",
    "    def plot_levels(ax, levels_data, title, color, is_dequant_plot=False):\n",
    "        unique_vals, counts = torch.unique(levels_data, return_counts=True)\n",
    "        bar_width = 0.8 \n",
    "        if len(unique_vals) > 1:\n",
    "            min_diff = (torch.sort(unique_vals).values[1:] - torch.sort(unique_vals).values[:-1]).min().item()\n",
    "            if is_dequant_plot or len(unique_vals) > 2 * (2**max(bw,bx,ba)): \n",
    "                ax.hist(levels_data.cpu().numpy(), bins=50, color=color, alpha=0.8, density=True)\n",
    "            else: \n",
    "                bar_width = min_diff * 0.8 if min_diff > 1e-6 else 0.8 \n",
    "                if bar_width < 1e-5: bar_width = 0.05 * (unique_vals.abs().mean().item() if len(unique_vals)>0 else 1.0)\n",
    "                if bar_width < 1e-5: bar_width = 0.05\n",
    "                ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                       width=bar_width, color=color, alpha=0.85)\n",
    "        elif len(unique_vals) == 1: \n",
    "             bar_width = 0.05 * abs(unique_vals.item()) if abs(unique_vals.item()) > 0 else 0.05\n",
    "             if bar_width == 0: bar_width = 0.05\n",
    "             ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                    width=bar_width, color=color, alpha=0.85)\n",
    "        else: \n",
    "            ax.text(0.5, 0.5, \"No data/bins\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(f\"{title}\\nUnique Bins/Values: {len(unique_vals)}\")\n",
    "        ax.set_ylabel(\"Normalized Freq. / Density\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plot_levels(axes[1, 0], w_round_inter_dequant, f\"Intermed. Quant W ({bw}b, STE-Round)\\n(Dequantized)\", 'darkslateblue', is_dequant_plot=True)\n",
    "    plot_levels(axes[1, 1], x_round_inter_dequant, f\"Intermed. Quant X ({bx}b, STE-Round)\\n(Dequantized)\", 'seagreen', is_dequant_plot=True)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    plot_levels(axes[2, 0], product_std_quant_levels, f\"Std. Product Quant Levels ({ba}-bit)\\n(Affine, torch.round)\", 'purple')\n",
    "    plot_levels(axes[2, 1], adc_orig_floor_output_levels, f\"Orig. ADC Output ({ba}-bit)\\n(Pre w,x STE-Floor)\", 'teal')\n",
    "    axes[2, 2].axis('off') \n",
    "\n",
    "    plot_levels(axes[3, 0], adc_orig_round_output_levels, f\"Orig. ADC Output ({ba}-bit)\\n(Pre w,x STE-Round)\", 'orangered')\n",
    "    plot_levels(axes[3, 1], adc_ashift_round_output_levels, f\"Ashift ADC Output ({ba}-bit)\\n(Pre w,x STE-Round)\", 'saddlebrown') \n",
    "    axes[3, 2].axis('off')\n",
    "    \n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlabel(\"Value / Level\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92]) \n",
    "    output_dir = os.path.join(\"ADC\", \"analysis_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # Include plot_suffix in filename\n",
    "    filename_core = f\"product_quant_compare_w{bw}x{bx}_out{ba}_k{k_adc}\"\n",
    "    plot_filename = os.path.join(output_dir, f\"{filename_core}{plot_suffix}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"\\nPlot saved to {plot_filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Quantization Comparison ( normal weights) ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 165\n",
      "    Orig. ADC (pre w,x STE-Floor) bins: 2\n",
      "    Orig. ADC (pre w,x STE-Round) bins: 2\n",
      "    Ashift ADC (pre w,x STE-Round) bins: 2\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 204\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 168\n",
      "    Intermediate W (symm, STE-Round, 8-bit) bins: 205\n",
      "    Intermediate X (aff, STE-Round, 8-bit) bins: 171\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4_normal_weights.png\n",
      "\n",
      "--- Product Quantization Comparison ( laplace weights) ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 135\n",
      "    Orig. ADC (pre w,x STE-Floor) bins: 2\n",
      "    Orig. ADC (pre w,x STE-Round) bins: 2\n",
      "    Ashift ADC (pre w,x STE-Round) bins: 2\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 165\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 182\n",
      "    Intermediate W (symm, STE-Round, 8-bit) bins: 160\n",
      "    Intermediate X (aff, STE-Round, 8-bit) bins: 184\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4_laplace_weights.png\n",
      "\n",
      "--- Product Quantization Comparison ( bimodal weights) ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 165\n",
      "    Orig. ADC (pre w,x STE-Floor) bins: 6\n",
      "    Orig. ADC (pre w,x STE-Round) bins: 6\n",
      "    Ashift ADC (pre w,x STE-Round) bins: 10\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 251\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 184\n",
      "    Intermediate W (symm, STE-Round, 8-bit) bins: 252\n",
      "    Intermediate X (aff, STE-Round, 8-bit) bins: 184\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4_bimodal_weights.png\n"
     ]
    }
   ],
   "source": [
    "common_params = {'bw': 8, 'bx': 8, 'ba': 8, 'k_adc': 4, 'num_samples': 10000}\n",
    "    \n",
    "run_product_quantization_comparison(generate_normal_data_for_weights, **common_params, plot_suffix=\"_normal_weights\")\n",
    "run_product_quantization_comparison(generate_laplace_data_for_weights, **common_params, plot_suffix=\"_laplace_weights\")\n",
    "run_product_quantization_comparison(generate_u_shaped_weights, **common_params, plot_suffix=\"_bimodal_weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
