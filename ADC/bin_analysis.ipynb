{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ADC.ste import ste_round, ste_floor # Import from your ste.py\n",
    "from scipy.stats import laplace, t # For Laplace and Student's t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_uniform(x, n_bits, ste_function=None, data_min_val=None, data_max_val=None):\n",
    "    \"\"\"\n",
    "    Performs uniform quantization on the input tensor x over a pre-defined range.\n",
    "    Returns dequantized values.\n",
    "    \"\"\"\n",
    "    if n_bits < 1:\n",
    "        raise ValueError(\"n_bits must be >= 1 for this quantization function.\")\n",
    "    if data_min_val is None or data_max_val is None:\n",
    "        raise ValueError(\"data_min_val and data_max_val must be provided.\")\n",
    "\n",
    "    data_min = torch.tensor(data_min_val, device=x.device, dtype=x.dtype)\n",
    "    data_max = torch.tensor(data_max_val, device=x.device, dtype=x.dtype)\n",
    "    \n",
    "    num_levels = 2**n_bits\n",
    "    \n",
    "    if num_levels <= 1 or (data_max - data_min).abs() < 1e-9:\n",
    "        # If only one level or zero range, all values quantize to data_min\n",
    "        # This also handles data_max == data_min gracefully for num_levels > 1\n",
    "        return torch.full_like(x, data_min)\n",
    "\n",
    "    scale = (data_max - data_min) / (num_levels - 1)\n",
    "    \n",
    "    x_clamped_input = torch.clamp(x, data_min, data_max)\n",
    "    # Transform to [0, num_levels - 1] range for quantization\n",
    "    x_transformed = (x_clamped_input - data_min) / scale\n",
    "    \n",
    "    if ste_function:\n",
    "        quantized_levels_float = ste_function(x_transformed)\n",
    "    else:\n",
    "        quantized_levels_float = torch.round(x_transformed) \n",
    "    \n",
    "    quantized_levels_int = torch.clamp(quantized_levels_float, 0, num_levels - 1)\n",
    "    \n",
    "    x_dequant = quantized_levels_int * scale + data_min\n",
    "    \n",
    "    return x_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_data(num_samples, mean=0.0, std=0.1): # Typical for weights\n",
    "    return torch.randn(num_samples) * std + mean, f\"Normal Weights (mean={mean:.2f}, std={std:.2f})\"\n",
    "\n",
    "def generate_exponential_data(num_samples, rate=1.0): # Typical for ReLU activations\n",
    "    # Exponential Pytorch takes rate = 1/scale. Scale is mean.\n",
    "    return torch.distributions.Exponential(rate).sample((num_samples,)), f\"Exponential Activations (rate={rate:.1f})\"\n",
    "\n",
    "def generate_uniform_data(num_samples, low=-2.0, high=2.0):\n",
    "    return torch.rand(num_samples) * (high - low) + low, f\"Uniform (low={low}, high={high})\"\n",
    "\n",
    "def generate_student_t_data(num_samples, df=3, loc=0.0, scale=1.0):\n",
    "    return torch.tensor(t.rvs(df=df, loc=loc, scale=scale, size=num_samples), dtype=torch.float32), \\\n",
    "           f\"Student-t (df={df}, loc={loc}, scale={scale})\"\n",
    "\n",
    "def generate_laplace_data(num_samples, loc=0.0, scale=1.0): \n",
    "    return torch.tensor(laplace.rvs(loc=loc, scale=scale, size=num_samples), dtype=torch.float32), \\\n",
    "           f\"Laplace (loc={loc}, scale={scale})\"\n",
    "\n",
    "def generate_bimodal_data(num_samples, mean1=-2.0, std1=0.5, mean2=2.0, std2=0.5):\n",
    "    n1 = num_samples // 2\n",
    "    n2 = num_samples - n1\n",
    "    data1 = torch.randn(n1) * std1 + mean1\n",
    "    data2 = torch.randn(n2) * std2 + mean2\n",
    "    return torch.cat((data1, data2)), f\"Bimodal (m1={mean1},s1={std1}; m2={mean2},s2={std2})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Neural Element Quantization Analysis (Weights & Activations) ---\n",
      "    n_bits = 4, Samples = 10000\n",
      "    Normal Weights (mean=0.00, std=0.05): Symmetric Range [-0.1841, 0.1841]\n",
      "    Exponential Activations (rate=0.5): Affine Range [0.0003, 16.3702]\n",
      "\n",
      "  --- Weights Bin Analysis ---\n",
      "    Normal Weights (mean=0.00, std=0.05): Original data, mean=0.000, std=0.050\n",
      "    W Quant (Std Round, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "    W Quant (ADC STE-Floor, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "    W Quant (ADC STE-Round, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "\n",
      "  --- Activations Bin Analysis ---\n",
      "    Exponential Activations (rate=0.5): Original data, mean=1.979, std=1.972\n",
      "    X Quant (Std Round, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "    X Quant (ADC STE-Floor, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "    X Quant (ADC STE-Round, 4-bit): Unique Bins = 16 (Max expected for W/X: 16)\n",
      "\n",
      "  --- Products (W_dequant * X_dequant) Bin Analysis ---\n",
      "    Product (Std Round, 4-bit W&X): Unique Bins = 157 (Max expected for W/X: 16)\n",
      "    Product (ADC STE-Floor, 4-bit W&X): Unique Bins = 147 (Max expected for W/X: 16)\n",
      "    Product (ADC STE-Round, 4-bit W&X): Unique Bins = 157 (Max expected for W/X: 16)\n",
      "\n",
      "Plot saved to ADC/analysis_results/neural_elem_quant_nbits4.png\n"
     ]
    }
   ],
   "source": [
    "n_bits=4\n",
    "num_samples=10000\n",
    "print(f\"\\n--- Neural Element Quantization Analysis (Weights & Activations) ---\")\n",
    "print(f\"    n_bits = {n_bits}, Samples = {num_samples}\")\n",
    "\n",
    "weights_data, weights_dist_name = generate_normal_data(num_samples, std=0.05) # Smaller std for weights\n",
    "activations_data, activations_dist_name = generate_exponential_data(num_samples, rate=0.5) # rate makes mean 1/rate=2\n",
    "\n",
    "# Determine ranges\n",
    "# Weights: Symmetric\n",
    "w_abs_max = weights_data.abs().max().item()\n",
    "w_min_val_sym, w_max_val_sym = -w_abs_max, w_abs_max\n",
    "if w_max_val_sym <= w_min_val_sym + 1e-9 : # Avoid zero range for constant data\n",
    "    w_max_val_sym = w_min_val_sym + 1.0\n",
    "    w_min_val_sym = -w_max_val_sym\n",
    "\n",
    "\n",
    "# Activations: Affine\n",
    "x_min_val_aff, x_max_val_aff = activations_data.min().item(), activations_data.max().item()\n",
    "if x_max_val_aff <= x_min_val_aff + 1e-9 : # Avoid zero range\n",
    "        x_max_val_aff = x_min_val_aff + 1.0\n",
    "\n",
    "print(f\"    {weights_dist_name}: Symmetric Range [{w_min_val_sym:.4f}, {w_max_val_sym:.4f}]\")\n",
    "print(f\"    {activations_dist_name}: Affine Range [{x_min_val_aff:.4f}, {x_max_val_aff:.4f}]\")\n",
    "\n",
    "# Quantize Weights\n",
    "w_q_std_round = quantize_uniform(weights_data, n_bits, ste_function=None, data_min_val=w_min_val_sym, data_max_val=w_max_val_sym)\n",
    "w_q_adc_ste_floor = quantize_uniform(weights_data, n_bits, ste_function=ste_floor, data_min_val=w_min_val_sym, data_max_val=w_max_val_sym)\n",
    "w_q_adc_ste_round = quantize_uniform(weights_data, n_bits, ste_function=ste_round, data_min_val=w_min_val_sym, data_max_val=w_max_val_sym)\n",
    "\n",
    "# Quantize Activations\n",
    "x_q_std_round = quantize_uniform(activations_data, n_bits, ste_function=None, data_min_val=x_min_val_aff, data_max_val=x_max_val_aff)\n",
    "x_q_adc_ste_floor = quantize_uniform(activations_data, n_bits, ste_function=ste_floor, data_min_val=x_min_val_aff, data_max_val=x_max_val_aff)\n",
    "x_q_adc_ste_round = quantize_uniform(activations_data, n_bits, ste_function=ste_round, data_min_val=x_min_val_aff, data_max_val=x_max_val_aff)\n",
    "\n",
    "# Products of dequantized values\n",
    "y_std_round = w_q_std_round * x_q_std_round\n",
    "y_adc_ste_floor = w_q_adc_ste_floor * x_q_adc_ste_floor\n",
    "y_adc_ste_round = w_q_adc_ste_round * x_q_adc_ste_round\n",
    "\n",
    "quantized_data_collections = {\n",
    "    \"Weights\": [\n",
    "        (\"Orig.\", weights_data, weights_dist_name),\n",
    "        (\"Std Round\", w_q_std_round, f\"W Quant (Std Round, {n_bits}-bit)\"),\n",
    "        (\"ADC STE-Floor\", w_q_adc_ste_floor, f\"W Quant (ADC STE-Floor, {n_bits}-bit)\"),\n",
    "        (\"ADC STE-Round\", w_q_adc_ste_round, f\"W Quant (ADC STE-Round, {n_bits}-bit)\"),\n",
    "    ],\n",
    "    \"Activations\": [\n",
    "        (\"Orig.\", activations_data, activations_dist_name),\n",
    "        (\"Std Round\", x_q_std_round, f\"X Quant (Std Round, {n_bits}-bit)\"),\n",
    "        (\"ADC STE-Floor\", x_q_adc_ste_floor, f\"X Quant (ADC STE-Floor, {n_bits}-bit)\"),\n",
    "        (\"ADC STE-Round\", x_q_adc_ste_round, f\"X Quant (ADC STE-Round, {n_bits}-bit)\"),\n",
    "    ],\n",
    "    \"Products (W_dequant * X_dequant)\": [\n",
    "        (\"Std Round\", y_std_round, f\"Product (Std Round, {n_bits}-bit W&X)\"),\n",
    "        (\"ADC STE-Floor\", y_adc_ste_floor, f\"Product (ADC STE-Floor, {n_bits}-bit W&X)\"),\n",
    "        (\"ADC STE-Round\", y_adc_ste_round, f\"Product (ADC STE-Round, {n_bits}-bit W&X)\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- Bin Analysis (Printed) ---\n",
    "for data_type, collections in quantized_data_collections.items():\n",
    "    print(f\"\\n  --- {data_type} Bin Analysis ---\")\n",
    "    for name_suffix, data, title in collections:\n",
    "        if \"Orig.\" in name_suffix: # Skip original for bin count\n",
    "                print(f\"    {title}: Original data, mean={data.mean():.3f}, std={data.std():.3f}\")\n",
    "                continue\n",
    "        unique_bins = torch.unique(data)\n",
    "        num_unique = len(unique_bins)\n",
    "        print(f\"    {title}: Unique Bins = {num_unique} (Max expected for W/X: {2**n_bits})\")\n",
    "        # print(f\"      Bins: {np.round(unique_bins.cpu().numpy(), 4)}\") # Optional: print bins\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16)) # 4 rows for Orig W/X, Quant W, Quant X, Product\n",
    "fig.suptitle(f\"Neural Element Quantization Analysis ({n_bits}-bit, {num_samples} samples)\", fontsize=16)\n",
    "\n",
    "# Row 0: Original Data\n",
    "axes[0, 0].hist(weights_data.cpu().numpy(), bins=100, color='gray', alpha=0.8, density=True)\n",
    "axes[0, 0].set_title(weights_dist_name)\n",
    "axes[0, 0].grid(True)\n",
    "axes[0, 1].hist(activations_data.cpu().numpy(), bins=100, color='gray', alpha=0.8, density=True)\n",
    "axes[0, 1].set_title(activations_dist_name)\n",
    "axes[0, 1].grid(True)\n",
    "axes[0, 2].axis('off') # Empty subplot\n",
    "\n",
    "plot_configs = [\n",
    "    (w_q_std_round, \"W Quant (Std Round)\"),\n",
    "    (w_q_adc_ste_floor, \"W Quant (ADC STE-Floor)\"),\n",
    "    (w_q_adc_ste_round, \"W Quant (ADC STE-Round)\"),\n",
    "    (x_q_std_round, \"X Quant (Std Round)\"),\n",
    "    (x_q_adc_ste_floor, \"X Quant (ADC STE-Floor)\"),\n",
    "    (x_q_adc_ste_round, \"X Quant (ADC STE-Round)\"),\n",
    "    (y_std_round, \"Product (Std Round W*X)\"),\n",
    "    (y_adc_ste_floor, \"Product (ADC STE-Floor W*X)\"),\n",
    "    (y_adc_ste_round, \"Product (ADC STE-Round W*X)\"),\n",
    "]\n",
    "\n",
    "row_map = [1, 1, 1, 2, 2, 2, 3, 3, 3] # Maps plot_configs index to row in subplot\n",
    "col_map = [0, 1, 2, 0, 1, 2, 0, 1, 2] # Maps plot_configs index to col in subplot\n",
    "\n",
    "for i, (data, title) in enumerate(plot_configs):\n",
    "    ax = axes[row_map[i], col_map[i]]\n",
    "    unique_values = torch.unique(data)\n",
    "    num_unique_bins = len(unique_values)\n",
    "    \n",
    "    # For product plots, use histogram; for quantized W/X, use bar if few bins.\n",
    "    if \"Product\" in title or num_unique_bins > 2**n_bits + 5 : # Heuristic for histogram vs bar\n",
    "        ax.hist(data.cpu().numpy(), bins=50 if \"Product\" in title else num_unique_bins, alpha=0.7, density=True)\n",
    "    else:\n",
    "        counts = torch.stack([(data == v).sum() for v in unique_values])\n",
    "        # Determine bar width for quantized W/X\n",
    "        data_range = data.max() - data.min()\n",
    "        bar_width_val = data_range / (max(1,num_unique_bins) * 2.0) # Heuristic\n",
    "        if num_unique_bins > 1:\n",
    "            sorted_unique_vals = torch.sort(unique_values).values\n",
    "            min_diff = (sorted_unique_vals[1:] - sorted_unique_vals[:-1]).min().item()\n",
    "            if min_diff > 1e-6 : \n",
    "                    bar_width_val = min_diff * 0.7\n",
    "        \n",
    "        ax.bar(unique_values.cpu().numpy(), counts.cpu().numpy() / num_samples, width=bar_width_val, alpha=0.7, label=f'Bins: {num_unique_bins}')\n",
    "\n",
    "    ax.set_title(f\"{title}\\nUnique Vals: {num_unique_bins}\")\n",
    "    ax.set_ylabel(\"Density / Norm. Freq.\")\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "output_dir = os.path.join(\"ADC\", \"analysis_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plot_filename = os.path.join(output_dir, f\"neural_elem_quant_nbits{n_bits}.png\")\n",
    "plt.savefig(plot_filename)\n",
    "print(f\"\\nPlot saved to {plot_filename}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
