{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ADC.ste import ste_round, ste_floor # Import from your ste.py\n",
    "from ADC.quantizers import ADCQuantizer, ADCQuantizerAshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_uniform_core(x, n_bits, round_fn, data_min_val, data_max_val):\n",
    "    \"\"\"\n",
    "    Core uniform quantization logic. Returns dequantized values and integer levels.\n",
    "    \"\"\"\n",
    "    data_min = torch.tensor(data_min_val, device=x.device, dtype=x.dtype)\n",
    "    data_max = torch.tensor(data_max_val, device=x.device, dtype=x.dtype)\n",
    "    num_levels = 2**n_bits\n",
    "\n",
    "    if num_levels <= 1 or (data_max - data_min).abs() < 1e-9:\n",
    "        levels_int = torch.zeros_like(x, dtype=torch.float32) # float for consistency\n",
    "        dequant_val = torch.full_like(x, data_min)\n",
    "        return dequant_val, levels_int\n",
    "\n",
    "    scale = (data_max - data_min) / (num_levels - 1)\n",
    "    \n",
    "    x_clamped_input = torch.clamp(x, data_min, data_max)\n",
    "    x_transformed = (x_clamped_input - data_min) / scale\n",
    "    \n",
    "    quantized_levels_float = round_fn(x_transformed)\n",
    "    quantized_levels_int = torch.clamp(quantized_levels_float, 0, num_levels - 1)\n",
    "    \n",
    "    x_dequant = quantized_levels_int * scale + data_min\n",
    "    # Shift levels for symmetric to be centered around 0 if desired for interpretation\n",
    "    # For this analysis, levels 0 to N-1 are fine.\n",
    "    return x_dequant, quantized_levels_int.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_elementwise(data, n_bits, quant_type, round_fn):\n",
    "    if quant_type == 'affine':\n",
    "        min_val, max_val = data.min().item(), data.max().item()\n",
    "        if max_val <= min_val + 1e-9: max_val = min_val + 1.0\n",
    "    elif quant_type == 'symmetric':\n",
    "        abs_max = data.abs().max().item()\n",
    "        min_val, max_val = -abs_max, abs_max\n",
    "        if max_val <= min_val + 1e-9 : \n",
    "            min_val = -1.0 # Default small range if data is all zero\n",
    "            max_val = 1.0\n",
    "    else:\n",
    "        raise ValueError(\"quant_type must be 'affine' or 'symmetric'\")\n",
    "    \n",
    "    return quantize_uniform_core(data, n_bits, round_fn, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_data(num_samples, mean=0.0, std=0.1):\n",
    "    return torch.randn(num_samples) * std + mean, f\"Normal (μ={mean:.2f}, σ={std:.2f})\"\n",
    "\n",
    "def generate_exponential_data(num_samples, rate=1.0):\n",
    "    return torch.distributions.Exponential(rate).sample((num_samples,)), f\"Exponential (rate={rate:.1f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_product_quantization_comparison(bw=4, bx=4, ba=8, k_adc=4, num_samples=10000):\n",
    "    print(f\"\\n--- Product Quantization Comparison ---\")\n",
    "    print(f\"Params: w_bits={bw}, x_bits={bx}, final_product_bits={ba}, adc_k={k_adc}, Samples={num_samples}\")\n",
    "\n",
    "    w_orig, w_dist_name = generate_normal_data(num_samples, std=0.05)\n",
    "    x_orig, x_dist_name = generate_exponential_data(num_samples, rate=0.5) # Mean = 2\n",
    "\n",
    "    # --- Method 1: Standard Quantization of Product ---\n",
    "    w_std_dequant_for_prod, _ = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=torch.round)\n",
    "    x_std_dequant_for_prod, _ = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=torch.round)\n",
    "    product_for_std_quant = w_std_dequant_for_prod * x_std_dequant_for_prod\n",
    "    # Quantize the product itself to 'ba' bits using affine quantization and standard round\n",
    "    _, product_std_quant_levels = quantize_elementwise(product_for_std_quant, n_bits=ba, quant_type='affine', round_fn=torch.round)\n",
    "\n",
    "    # --- Method 2: ADC-Style (STE-Floor for w,x pre-quant) ---\n",
    "    w_floor_inter_dequant, w_floor_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_floor)\n",
    "    x_floor_inter_dequant, x_floor_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_floor)\n",
    "    product_input_to_adc_floor = w_floor_inter_dequant * x_floor_inter_dequant\n",
    "    adc_module_floor = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) # M=1 for element-wise product\n",
    "    adc_floor_output_levels = adc_module_floor(product_input_to_adc_floor)\n",
    "\n",
    "    # --- Method 3: ADC-Style (STE-Round for w,x pre-quant) ---\n",
    "    w_round_inter_dequant, w_round_inter_levels = quantize_elementwise(w_orig, n_bits=bw, quant_type='symmetric', round_fn=ste_round)\n",
    "    x_round_inter_dequant, x_round_inter_levels = quantize_elementwise(x_orig, n_bits=bx, quant_type='affine', round_fn=ste_round)\n",
    "    product_input_to_adc_round = w_round_inter_dequant * x_round_inter_dequant\n",
    "    adc_module_round = ADCQuantizer(M=1, bx=bx, bw=bw, ba=ba, k=k_adc) \n",
    "    adc_round_output_levels = adc_module_round(product_input_to_adc_round)\n",
    "    \n",
    "    print(\"\\n  --- Product Bin Counts (all target 'ba' bits) ---\")\n",
    "    print(f\"    Standard Product Quant (affine, round) bins: {len(torch.unique(product_std_quant_levels))}\")\n",
    "    print(f\"    ADC Output (pre w,x quant w/ STE-Floor) bins: {len(torch.unique(adc_floor_output_levels))}\")\n",
    "    print(f\"    ADC Output (pre w,x quant w/ STE-Round) bins: {len(torch.unique(adc_round_output_levels))}\")\n",
    "    print(f\"  --- Intermediate Quantized W/X Bin Counts ---\")\n",
    "    print(f\"    Intermediate W (symm, STE-Floor, {bw}-bit) bins: {len(torch.unique(w_floor_inter_levels))}\")\n",
    "    print(f\"    Intermediate X (aff, STE-Floor, {bx}-bit) bins: {len(torch.unique(x_floor_inter_levels))}\")\n",
    "\n",
    "    # --- Visualization ---\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 16)) # Now 3 rows\n",
    "    title_str = (f\"Product Quantization Bin Comparison ({ba}-bit output)\\n\"\n",
    "                 f\"Inputs: w ({bw}b-symm), x ({bx}b-aff). ADC M=1, k={k_adc}\")\n",
    "    fig.suptitle(title_str, fontsize=15)\n",
    "\n",
    "    # Row 0: Originals \n",
    "    axes[0, 0].hist(w_orig.cpu().numpy(), bins=100, color='gray', alpha=0.7, density=True)\n",
    "    axes[0, 0].set_title(f\"Original W ({w_dist_name})\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    axes[0, 1].hist(x_orig.cpu().numpy(), bins=100, color='skyblue', alpha=0.7, density=True)\n",
    "    axes[0, 1].set_title(f\"Original X ({x_dist_name})\")\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 2].axis('off') # Keep one empty for layout or future use\n",
    "\n",
    "    # Helper for plotting levels (bins)\n",
    "    def plot_levels(ax, levels_data, title, color, is_dequant_plot=False):\n",
    "        unique_vals, counts = torch.unique(levels_data, return_counts=True)\n",
    "        bar_width = 0.8 \n",
    "        if len(unique_vals) > 1:\n",
    "            min_diff = (torch.sort(unique_vals).values[1:] - torch.sort(unique_vals).values[:-1]).min().item()\n",
    "            # For dequantized values, which are continuous-like, or many bins, use hist\n",
    "            if is_dequant_plot or len(unique_vals) > 2 * (2**max(bw,bx,ba)): # Heuristic for hist\n",
    "                ax.hist(levels_data.cpu().numpy(), bins=50, color=color, alpha=0.8, density=True)\n",
    "            else: # For few discrete levels, use bar\n",
    "                bar_width = min_diff * 0.8 if min_diff > 1e-6 else 0.8 # Ensure positive width for bars\n",
    "                if bar_width < 1e-5: bar_width = 0.05 * (unique_vals.abs().mean().item() if len(unique_vals)>0 else 1.0)\n",
    "                if bar_width < 1e-5: bar_width = 0.05\n",
    "                ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                       width=bar_width, color=color, alpha=0.85)\n",
    "        elif len(unique_vals) == 1: \n",
    "             bar_width = 0.05 * abs(unique_vals.item()) if abs(unique_vals.item()) > 0 else 0.05\n",
    "             if bar_width == 0: bar_width = 0.05\n",
    "             ax.bar(unique_vals.cpu().numpy(), (counts.cpu().numpy() / num_samples), \n",
    "                    width=bar_width, color=color, alpha=0.85)\n",
    "        else: # No data or no unique values\n",
    "            ax.text(0.5, 0.5, \"No data/bins\", ha=\"center\", va=\"center\")\n",
    "\n",
    "\n",
    "        ax.set_title(f\"{title}\\nUnique Bins/Values: {len(unique_vals)}\")\n",
    "        ax.set_ylabel(\"Normalized Freq. / Density\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Row 1: Intermediately Quantized W, X (using STE-Floor for ADC path example) and a Product Input\n",
    "    # Plotting dequantized values here to see their distribution before product\n",
    "    plot_levels(axes[1, 0], w_floor_inter_dequant, f\"Intermed. Quant W ({bw}b, STE-Floor)\\n(Dequantized)\", 'darkblue', is_dequant_plot=True)\n",
    "    plot_levels(axes[1, 1], x_floor_inter_dequant, f\"Intermed. Quant X ({bx}b, STE-Floor)\\n(Dequantized)\", 'darkgreen', is_dequant_plot=True)\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "\n",
    "    # Row 2: Final Product Quantized Levels (Bins) from the three methods\n",
    "    plot_levels(axes[2, 0], product_std_quant_levels, f\"Std. Product Quant Levels ({ba}-bit)\\n(Affine, torch.round)\", 'purple')\n",
    "    plot_levels(axes[2, 1], adc_floor_output_levels, f\"ADC Output Levels ({ba}-bit)\\n(Pre w,x STE-Floor)\", 'teal')\n",
    "    plot_levels(axes[2, 2], adc_round_output_levels, f\"ADC Output Levels ({ba}-bit)\\n(Pre w,x STE-Round)\", 'orangered')\n",
    "    \n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.set_xlabel(\"Value / Level\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92]) \n",
    "    output_dir = os.path.join(\"ADC\", \"analysis_results\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    plot_filename = os.path.join(output_dir, f\"product_quant_compare_w{bw}x{bx}_out{ba}_k{k_adc}.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"\\nPlot saved to {plot_filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Quantization Comparison ---\n",
      "Params: w_bits=8, x_bits=8, final_product_bits=8, adc_k=4, Samples=10000\n",
      "\n",
      "  --- Product Bin Counts (all target 'ba' bits) ---\n",
      "    Standard Product Quant (affine, round) bins: 144\n",
      "    ADC Output (pre w,x quant w/ STE-Floor) bins: 2\n",
      "    ADC Output (pre w,x quant w/ STE-Round) bins: 2\n",
      "  --- Intermediate Quantized W/X Bin Counts ---\n",
      "    Intermediate W (symm, STE-Floor, 8-bit) bins: 186\n",
      "    Intermediate X (aff, STE-Floor, 8-bit) bins: 153\n",
      "\n",
      "Plot saved to ADC/analysis_results/product_quant_compare_w8x8_out8_k4.png\n"
     ]
    }
   ],
   "source": [
    "run_product_quantization_comparison(bw=8, bx=8, ba=8, k_adc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
